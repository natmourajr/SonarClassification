{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db1e8ed",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61f352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673cc4f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d63c88a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Label</th>\n",
       "      <th>Raw Data Path</th>\n",
       "      <th>Info Files Path</th>\n",
       "      <th>MFCC Processed Path</th>\n",
       "      <th>Classes Name Path</th>\n",
       "      <th>ID 2 Class Path</th>\n",
       "      <th>Ready to Train Data Path</th>\n",
       "      <th>Indexes Path</th>\n",
       "      <th>Number of Folds</th>\n",
       "      <th>Pipeline Path</th>\n",
       "      <th>Model Path</th>\n",
       "      <th>Hidden Neurons Path</th>\n",
       "      <th>Number of Inits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ShipsEar_NN_Classification</td>\n",
       "      <td>../data/shipsEar_AUDIOS</td>\n",
       "      <td>../data/file_info.csv</td>\n",
       "      <td>../data/mfcc_data.csv</td>\n",
       "      <td>../data/class_names.csv</td>\n",
       "      <td>../data/id2classes.csv</td>\n",
       "      <td>../data/train_data.csv</td>\n",
       "      <td>../data/indexes</td>\n",
       "      <td>5</td>\n",
       "      <td>../data/pipelines</td>\n",
       "      <td>../data/models</td>\n",
       "      <td>../data/models/hidden_neurons.pkl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Train Label            Raw Data Path        Info Files Path  \\\n",
       "0  ShipsEar_NN_Classification  ../data/shipsEar_AUDIOS  ../data/file_info.csv   \n",
       "\n",
       "     MFCC Processed Path        Classes Name Path         ID 2 Class Path  \\\n",
       "0  ../data/mfcc_data.csv  ../data/class_names.csv  ../data/id2classes.csv   \n",
       "\n",
       "  Ready to Train Data Path     Indexes Path  Number of Folds  \\\n",
       "0   ../data/train_data.csv  ../data/indexes                5   \n",
       "\n",
       "       Pipeline Path      Model Path                Hidden Neurons Path  \\\n",
       "0  ../data/pipelines  ../data/models  ../data/models/hidden_neurons.pkl   \n",
       "\n",
       "   Number of Inits  \n",
       "0                1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_config = pd.read_csv('../data/config.csv')\n",
    "df_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f58e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(df_config['Ready to Train Data Path'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a57ede",
   "metadata": {},
   "source": [
    "## Fit a preprocessing pipeline for each kFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d514943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "index_path = df_config['Indexes Path'][0]\n",
    "pipe_path = df_config['Pipeline Path'][0]\n",
    "n_folds =  df_config['Number of Folds'][0]\n",
    "\n",
    "data = df_train.drop(columns=['target'])\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    index_file = 'kFoldsCV_fold_%i_of_%i_indexes.pkl'%(ifold, n_folds)\n",
    "    with open(os.path.join(index_path,index_file),'rb') as file_handler:\n",
    "            [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "    \n",
    "    # criando o pipeline\n",
    "    pipe = Pipeline(steps=[(\"scaler\", MinMaxScaler())])\n",
    "    pipe.fit(data.loc[trn_idx,:])\n",
    "    \n",
    "    pipe_name = 'kFoldsCV_fold_%i_of_%i_pipe.pkl'%(ifold, n_folds)\n",
    "    with open(os.path.join(pipe_path,pipe_name),'wb') as file_handler:\n",
    "        joblib.dump(pipe, file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f34dd3",
   "metadata": {},
   "source": [
    "# Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8394424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel:\n",
    "    def __init__(self, n_hidden_neurons=2, verbose=2):\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.model = None\n",
    "        self.trn_history = None\n",
    "        self.trained = False\n",
    "        self.verbose = verbose\n",
    "    def __str__(self):\n",
    "        m_str = 'Class MLPModel\\n'\n",
    "        if self.trained:\n",
    "            m_str += 'Model is fitted, '\n",
    "        else:\n",
    "            m_str += 'Model is not fitted, '\n",
    "        m_str += 'instance created with %i hidden neurons'%(self.n_hidden_neurons) \n",
    "        return m_str\n",
    "    def create_model(self, data, target, random_state=0, learning_rate=0.01):\n",
    "        #tf.random.set_seed(random_state)\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # add a input to isolate the input of NN model\n",
    "        model.add(tf.keras.Input(shape=(data.shape[1],)))\n",
    "        # add a non-linear single neuron layer\n",
    "        hidden_layer = layers.Dense(units=self.n_hidden_neurons,\n",
    "                                    activation='tanh',\n",
    "                                    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                                    kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                                    bias_regularizer=regularizers.L2(1e-4),\n",
    "                                    bias_initializer=initializers.Zeros()\n",
    "                                   )\n",
    "        model.add(hidden_layer)\n",
    "        # add a non-linear output layer with max sparse target shape\n",
    "        output_layer = layers.Dense(units=target.shape[1],\n",
    "                                    activation='tanh',\n",
    "                                    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                                    bias_initializer=initializers.Zeros()\n",
    "                                   )\n",
    "        model.add(output_layer)\n",
    "        # creating a optimization function using steepest gradient\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                                  decay_steps=100,\n",
    "                                                                  decay_rate=0.9)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "        # compile the model\n",
    "        loss = keras.losses.CategoricalCrossentropy(from_logits=False,\n",
    "                                                    label_smoothing=0.0,\n",
    "                                                    axis=-1,\n",
    "                                                    reduction=\"auto\",\n",
    "                                                    name=\"cat_crossent\",)\n",
    "\n",
    "\n",
    "        cat_cross = keras.losses.BinaryCrossentropy(from_logits=False,\n",
    "                                                         label_smoothing=0.0,\n",
    "                                                         axis=-1,\n",
    "                                                         reduction=\"auto\",\n",
    "                                                         name=\"cat_crossent_met\",)\n",
    "        cat_acc_metric = keras.metrics.BinaryAccuracy(name=\"cat_acc\", dtype=None)\n",
    "        acc_metric = keras.metrics.Accuracy(name=\"accuracy\",dtype=None)\n",
    "        mse_metric = keras.metrics.MeanSquaredError(name=\"mse\", dtype=None)\n",
    "        rmse_metric = keras.metrics.RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
    "\n",
    "        model.compile(loss=\"mean_squared_error\", \n",
    "                      optimizer=optimizer,\n",
    "                      metrics=[acc_metric,mse_metric,rmse_metric])\n",
    "        return model\n",
    "    def fit(self, X, Y,\n",
    "            trn_id=None, \n",
    "            val_id=None, \n",
    "            epochs=50,\n",
    "            batch_size=4,\n",
    "            patience = 100,\n",
    "            learning_rate=0.01, random_state=0):\n",
    "        \n",
    "        X_copy = X.copy()\n",
    "        Y_copy = Y.copy()\n",
    "        \n",
    "        model = self.create_model(X_copy,Y_copy, random_state=random_state, learning_rate=learning_rate)\n",
    "        \n",
    "        # early stopping to avoid overtraining\n",
    "        earlyStopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                patience=patience,verbose=self.verbose, \n",
    "                                                mode='auto')\n",
    "    \n",
    "        trn_desc = model.fit(X_copy[trn_id,:], Y_copy[trn_id],\n",
    "                             epochs=epochs,\n",
    "                             batch_size=batch_size,\n",
    "                             callbacks=[earlyStopping], \n",
    "                             verbose=self.verbose,\n",
    "                             validation_data=(X_copy[val_id,:],\n",
    "                                              Y_copy[val_id]),\n",
    "                            )\n",
    "        self.model = model\n",
    "        self.trn_history = trn_desc\n",
    "        self.trained = True\n",
    "    def predict(self, data):\n",
    "        return self.model.predict(data)\n",
    "    def save(self, file_path):\n",
    "        with open(file_path,'wb') as file_handler:\n",
    "            joblib.dump([self.n_hidden_neurons, self.model,\n",
    "                        self.trn_history, self.trained], file_handler)\n",
    "    def load(self, file_path):\n",
    "        with open(file_path,'rb') as file_handler:\n",
    "            [self.n_hidden_neurons, self.model, self.trn_history, self.trained]= joblib.load(file_handler)\n",
    "    def model_with_no_output_layer(self):\n",
    "        buffer_model = tf.keras.Sequential()    \n",
    "        # add a input to isolate the input of NN model\n",
    "        buffer_model.add(tf.keras.Input(shape=(model.model.layers[0].get_weights()[0].shape[0],)))\n",
    "        # add a non-linear single neuron layer\n",
    "        hidden_layer = layers.Dense(units=model.model.layers[0].get_weights()[1].shape[0],\n",
    "                                    activation='tanh')\n",
    "        buffer_model.add(hidden_layer)    \n",
    "        output_layer = layers.Dense(units=1,activation='tanh')\n",
    "    \n",
    "        for idx, layer in enumerate(buffer_model.layers):\n",
    "            layer.set_weights(model.model.layers[idx].get_weights())\n",
    "        return buffer_model\n",
    "    def predict_one_layer_before_output(self, data):\n",
    "        buffer_model = self.model_with_no_output_layer()\n",
    "        return buffer_model.predict(data)\n",
    "\n",
    "    \n",
    "def write_list_of_hidden_neurons(filename, hidden_neurons):\n",
    "    with open(filename,'wb') as file_handler:\n",
    "        pickle.dump([hidden_neurons],file_handler)\n",
    "    return 0\n",
    "def get_list_of_hidden_neurons(filename):\n",
    "    with open(filename,'rb') as file_handler:\n",
    "        [hidden_neurons] = pickle.load(file_handler)\n",
    "    return hidden_neurons        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "385e77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descomente somente quando for necessÃ¡rio\n",
    "#hidden_neurons = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "#hidden_neurons = [1]\n",
    "#write_list_of_hidden_neurons(df_config['Hidden Neurons Path'][0],hidden_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029814be",
   "metadata": {},
   "source": [
    "# Config for kFold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "732ee6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kFold Files Generation\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = get_list_of_hidden_neurons(df_config['Hidden Neurons Path'][0])\n",
    "model_path = df_config['Model Path'][0]\n",
    "n_folds =  df_config['Number of Folds'][0]\n",
    "\n",
    "print('kFold Files Generation')\n",
    "\n",
    "buffer = 0*np.ones([n_folds,len(hidden_neurons)])\n",
    "df_buffer = pd.DataFrame(data=buffer, columns=hidden_neurons,index=range(n_folds))\n",
    "df_buffer.to_csv(os.path.join(model_path, '%s_kfold_model_acc.csv'%(df_config['Train Label'][0])))\n",
    "buffer = False*np.ones([n_folds,len(hidden_neurons)])\n",
    "df_buffer = pd.DataFrame(data=buffer, columns=hidden_neurons,index=range(n_folds))\n",
    "df_buffer.to_csv(os.path.join(model_path, '%s_kfold_model_status.csv'%(df_config['Train Label'][0])))\n",
    "buffer = False*np.ones([n_folds,len(hidden_neurons)])\n",
    "df_buffer = pd.DataFrame(data=buffer, columns=hidden_neurons,index=range(n_folds))\n",
    "df_buffer.to_csv(os.path.join(model_path, '%s_kfold_model_names.csv'%(df_config['Train Label'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2c4e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kFold Training Process\n",
      "Training Process 0 fold of 5 folds\n",
      "Training for 1 neuron in [1]\n",
      "ShipsEar_NN_Classification - Training Model with 1 neurons - 0 init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "115934/115934 - 148s - loss: 0.0640 - accuracy: 0.0000e+00 - mse: 0.0638 - rmse: 0.2527 - val_loss: 0.0683 - val_accuracy: 0.0000e+00 - val_mse: 0.0681 - val_rmse: 0.2610 - 148s/epoch - 1ms/step\n",
      "Epoch 2/1000\n",
      "115934/115934 - 159s - loss: 0.0640 - accuracy: 0.0000e+00 - mse: 0.0638 - rmse: 0.2526 - val_loss: 0.0683 - val_accuracy: 0.0000e+00 - val_mse: 0.0681 - val_rmse: 0.2610 - 159s/epoch - 1ms/step\n",
      "Epoch 3/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, model_name)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     model \u001b[38;5;241m=\u001b[39m MLPModel(n_hidden_neurons\u001b[38;5;241m=\u001b[39mineuron,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_trgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrn_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miinit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, model_name))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [4], line 85\u001b[0m, in \u001b[0;36mMLPModel.fit\u001b[0;34m(self, X, Y, trn_id, val_id, epochs, batch_size, patience, learning_rate, random_state)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# early stopping to avoid overtraining\u001b[39;00m\n\u001b[1;32m     81\u001b[0m earlyStopping \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     82\u001b[0m                                         patience\u001b[38;5;241m=\u001b[39mpatience,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \n\u001b[1;32m     83\u001b[0m                                         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m trn_desc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrn_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearlyStopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mY_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrn_history \u001b[38;5;241m=\u001b[39m trn_desc\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for kFolds CV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('kFold Training Process')\n",
    "\n",
    "if True: # remove when your are shure to spent time!!!\n",
    "    index_path = df_config['Indexes Path'][0]\n",
    "    pipe_path = df_config['Pipeline Path'][0]\n",
    "    model_path = df_config['Model Path'][0]\n",
    "\n",
    "    data = df_train.drop(columns=['target']).values\n",
    "    trgt = df_train['target'].values\n",
    "    \n",
    "    df_acc = pd.read_csv(os.path.join(model_path, '%s_kfold_model_acc.csv'%(df_config['Train Label'][0])),index_col=0)\n",
    "    df_status = pd.read_csv(os.path.join(model_path, '%s_kfold_model_status.csv'%(df_config['Train Label'][0])),index_col=0)\n",
    "    df_status = df_status.astype(bool)\n",
    "    df_names = pd.read_csv(os.path.join(model_path, '%s_kfold_model_names.csv'%(df_config['Train Label'][0])),index_col=0)\n",
    "    df_names = df_names.astype(str)\n",
    "\n",
    "    for ifold in range(n_folds):\n",
    "        print('Training Process %i fold of %i folds'%(ifold, n_folds))\n",
    "        cv_name = 'kFoldsCV_fold_%i_of_%i_indexes.pkl'%(ifold, n_folds)\n",
    "        with open(os.path.join(index_path,cv_name),'rb') as file_handler:\n",
    "            [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "        pipe_name = 'kFoldsCV_fold_%i_of_%i_pipe.pkl'%(ifold, n_folds)\n",
    "        with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "            pipe = joblib.load(file_handler)\n",
    "        \n",
    "        trn_data = pipe.transform(data)\n",
    "        trn_trgt = tf.keras.utils.to_categorical(trgt, num_classes=len(np.unique(trgt)))\n",
    "        \n",
    "        for ineuron in hidden_neurons:\n",
    "            print('Training for %i neuron in'%(ineuron),hidden_neurons)\n",
    "            best_acc = -1\n",
    "            best_init = -1\n",
    "            if df_status.loc[ifold,str(ineuron)] == False:\n",
    "                 for iinit in range(df_config['Number of Inits'][0]):\n",
    "                        print('%s - Training Model with %i neurons - %i init'%(df_config['Train Label'][0], ineuron, iinit))\n",
    "                        model_name = '%s_kFoldsCV_fold_%i_MLPModel_%i_hidden_neurons_%i_init.pkl'%(df_config['Train Label'][0], \n",
    "                                                                                                   ifold, ineuron,\n",
    "                                                                                                   iinit)\n",
    "                        if os.path.exists(os.path.join(model_path, model_name)) == False:\n",
    "                            model = MLPModel(n_hidden_neurons=ineuron,verbose=2)\n",
    "                            model.fit(trn_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, epochs=5, random_state=iinit)\n",
    "                            model.save(os.path.join(model_path, model_name))\n",
    "                        else:\n",
    "                            model = MLPModel(n_hidden_neurons=ineuron,verbose=0)\n",
    "                            model.load(os.path.join(model_path, model_name))\n",
    "                        model_acc = accuracy_score(trn_trgt,np.sign(model.predict(pipe.transform(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d2a2969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_trgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42806398",
   "metadata": {},
   "source": [
    "## Create a sklearn pipe por each basic MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb51146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing social\n",
      "Dataset shape: 54, 13\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_0_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_1_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_2_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_3_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Error Sum:  0.0\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_4_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "Processing clinical\n",
      "Dataset shape: 54, 16\n",
      "clinical - Loading Model with 1 neuron(s): clinical_kFoldsCV_fold_0_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "clinical - Loading Model with 1 neuron(s): clinical_kFoldsCV_fold_1_MLPModel_1_hidden_neurons_1_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "clinical - Loading Model with 1 neuron(s): clinical_kFoldsCV_fold_2_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "clinical - Loading Model with 1 neuron(s): clinical_kFoldsCV_fold_3_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Error Sum:  0.0\n",
      "clinical - Loading Model with 1 neuron(s): clinical_kFoldsCV_fold_4_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Error Sum:  0.0\n",
      "Processing pos_test\n",
      "Dataset shape: 54, 4\n",
      "pos_test - Loading Model with 1 neuron(s): pos_test_kFoldsCV_fold_0_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "pos_test - Loading Model with 1 neuron(s): pos_test_kFoldsCV_fold_1_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Error Sum:  0.0\n",
      "pos_test - Loading Model with 1 neuron(s): pos_test_kFoldsCV_fold_2_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "pos_test - Loading Model with 1 neuron(s): pos_test_kFoldsCV_fold_3_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "pos_test - Loading Model with 1 neuron(s): pos_test_kFoldsCV_fold_4_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n",
      "Processing personal\n",
      "Dataset shape: 54, 10\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_0_MLPModel_1_hidden_neurons_2_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Error Sum:  0.0\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_1_MLPModel_1_hidden_neurons_3_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Error Sum:  0.0\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_2_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Error Sum:  0.0\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_3_MLPModel_1_hidden_neurons_1_init.pkl\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Error Sum:  0.0\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_4_MLPModel_1_hidden_neurons_2_init.pkl\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Error Sum:  0.0\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "datasets_data = [dev_social, dev_clinical, dev_pos_test, dev_personal]\n",
    "datasets_name = ['social', 'clinical', 'pos_test', 'personal']\n",
    "datasets_n_neurons = [1, 1, 1, 1] \n",
    "model_path = '../data/models'\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "if True: # remove when your are shure to spent time!!!\n",
    "    cv_path = '../data/indexes'\n",
    "    pipe_path = '../data/pipelines'\n",
    "    model_path = '../data/models'\n",
    "\n",
    "    for idataset, dataset_name in enumerate(datasets_name): \n",
    "        print('Processing %s'%(dataset_name))\n",
    "        data = datasets_data[idataset]\n",
    "        print('Dataset shape: %i, %i'%(data.shape[0],data.shape[1]))\n",
    "        df_acc = pd.read_csv(os.path.join(model_path, '%s_kfold_model_acc.csv'%(dataset_name)),index_col=0)\n",
    "        df_status = pd.read_csv(os.path.join(model_path, '%s_kfold_model_status.csv'%(dataset_name)),index_col=0)\n",
    "        df_status = df_status.astype(bool)\n",
    "        df_names = pd.read_csv(os.path.join(model_path, '%s_kfold_model_names.csv'%(dataset_name)),index_col=0)\n",
    "        df_names = df_names.astype(str)\n",
    "\n",
    "        for ifold in range(n_folds):\n",
    "            cv_name = 'kFoldsCV_fold_%i_indexes.pkl'%(ifold)\n",
    "            with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "                [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "            pipe_name = 'kFoldsCV_fold_%i_pipe_%s.pkl'%(ifold, datasets_name[idataset])\n",
    "            with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "                pipe = joblib.load(file_handler)\n",
    "            trn_data = pipe.transform(data)\n",
    "            trn_trgt = dev_target.values\n",
    "            \n",
    "            # get the number of neurons choose in analysis notebook and process data\n",
    "            if df_status.loc[ifold,str(datasets_n_neurons[idataset])] != True:\n",
    "                print('Train the model')\n",
    "            else:\n",
    "                # load model\n",
    "                print('%s - Loading Model with %i neuron(s): %s'%(dataset_name, datasets_n_neurons[idataset], \n",
    "                                                                df_names.loc[ifold, str(datasets_n_neurons[idataset])]))\n",
    "                model_name = df_names.loc[ifold, str(datasets_n_neurons[idataset])]\n",
    "                if os.path.exists(os.path.join(model_path, model_name)) == False:\n",
    "                    print('Please do the training process!!!')\n",
    "                    continue\n",
    "                else:\n",
    "                    model = MLPModel(n_hidden_neurons=datasets_n_neurons[idataset],verbose=0)\n",
    "                    model.load(os.path.join(model_path, model_name))\n",
    "                    \n",
    "                \n",
    "                # add this model to a pipeline\n",
    "                # here I found a problem: when I tried to save a new pipeline, thinks start get weird!\n",
    "                # I realise that I need to load the model and after that append it into the pipeline\n",
    "                \n",
    "                new_pipe = deepcopy(pipe)\n",
    "                new_pipe.steps.append(('nn_%s'%(dataset_name), model.model))\n",
    "                # for test\n",
    "                #output_pipe = pipe.predict(data) # error because one of the previous steps has no predict within\n",
    "                transformed_data = new_pipe[:-1].transform(data)\n",
    "                output_pipe = new_pipe[-1].predict(transformed_data)\n",
    "                output_model = model.model.predict(pipe.transform(data))\n",
    "                print('Error Sum: ',np.sum(output_pipe-output_model))\n",
    "                #pipe_nn_name = 'kFoldsCV_fold_%i_pipe+basicNN_%s.pkl'%(ifold, datasets_name[idataset])\n",
    "                #joblib.dump(new_pipe,os.path.join(model_path, pipe_nn_name))\n",
    "                \n",
    "                #new_pipe = deepcopy(pipe)\n",
    "                #new_pipe.steps.append(('nn_%s_no_output'%(dataset_name), model.model_with_no_output_layer()))\n",
    "                #pipe_nn_name = 'kFoldsCV_fold_%i_pipe+basicNN_no_output_%s.pkl'%(ifold, datasets_name[idataset])\n",
    "                #joblib.dump(new_pipe,os.path.join(model_path, pipe_nn_name))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74287de",
   "metadata": {},
   "source": [
    "## Making model fusion\n",
    "\n",
    "The fusion process will be using the following struct\n",
    "\n",
    "1 - Social+Personal => SoPe Step\n",
    "\n",
    "2 - SoPe + Clinical => SoPeCli\n",
    "\n",
    "3 - SoPeCli+Pos-Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13856541",
   "metadata": {},
   "source": [
    "### Social + Personal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66141d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kFold Config Files Generation\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = [1, 2, 3, 4, 5, 6] # first hint!\n",
    "model_path = '../data/models'\n",
    "n_folds = 5\n",
    "datasets_n_neurons = [1, 1, 1, 1] # check the chose in analysis notebook\n",
    "\n",
    "print('kFold Config Files Generation')\n",
    "if True: # remove when your are shure to spent time!!!    \n",
    "    datasets_name = ['social+personal']\n",
    "    metrics = ['acc', 'sens', 'spec', 'sp']\n",
    "\n",
    "    buffer = 0*np.ones([n_folds,len(hidden_neurons)])\n",
    "    df_buffer = pd.DataFrame(data=buffer, columns=hidden_neurons,index=range(n_folds))\n",
    "\n",
    "    for idataset, dataset_name in enumerate(datasets_name):\n",
    "        for imetric in metrics:\n",
    "            df_buffer.to_csv(os.path.join(model_path, '%s_kfold_model_%s.csv'%(dataset_name,imetric)))\n",
    "    buffer = False*np.ones([n_folds,len(hidden_neurons)])\n",
    "    df_buffer = pd.DataFrame(data=buffer, columns=hidden_neurons,index=range(n_folds))\n",
    "    df_buffer.to_csv(os.path.join(model_path, '%s_kfold_model_status.csv'%(dataset_name)))\n",
    "    buffer = False*np.ones([n_folds,len(hidden_neurons)])\n",
    "    df_buffer = pd.DataFrame(data=buffer, columns=hidden_neurons,index=range(n_folds))\n",
    "    df_buffer.to_csv(os.path.join(model_path, '%s_kfold_model_names.csv'%(dataset_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc377eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kFold Training Process\n",
      "Fold 0 - Processing basic datasets\n",
      "Fold 0 - Processing social\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_0_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Fold 0 - Processing personal\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_0_MLPModel_1_hidden_neurons_2_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Fold 1 - Processing basic datasets\n",
      "Fold 1 - Processing social\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_1_MLPModel_1_hidden_neurons_0_init.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-83924517abb9>:52: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if fusioned_data == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Fold 1 - Processing personal\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_1_MLPModel_1_hidden_neurons_3_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Fold 2 - Processing basic datasets\n",
      "Fold 2 - Processing social\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_2_MLPModel_1_hidden_neurons_0_init.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-83924517abb9>:52: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if fusioned_data == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Fold 2 - Processing personal\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_2_MLPModel_1_hidden_neurons_0_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Fold 3 - Processing basic datasets\n",
      "Fold 3 - Processing social\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_3_MLPModel_1_hidden_neurons_0_init.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-83924517abb9>:52: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if fusioned_data == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Fold 3 - Processing personal\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_3_MLPModel_1_hidden_neurons_1_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Fold 4 - Processing basic datasets\n",
      "Fold 4 - Processing social\n",
      "social - Loading Model with 1 neuron(s): social_kFoldsCV_fold_4_MLPModel_1_hidden_neurons_0_init.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-83924517abb9>:52: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if fusioned_data == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Fold 4 - Processing personal\n",
      "personal - Loading Model with 1 neuron(s): personal_kFoldsCV_fold_4_MLPModel_1_hidden_neurons_2_init.pkl\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "social+personal - Training Model with 1 neurons - 0 init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-83924517abb9>:52: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if fusioned_data == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "Acc (1 hidden neurons, 0 init):  0.8888888888888888\n",
      "social+personal - Training Model with 2 neurons - 0 init\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Acc (2 hidden neurons, 0 init):  0.8888888888888888\n",
      "social+personal - Training Model with 3 neurons - 0 init\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Acc (3 hidden neurons, 0 init):  0.8888888888888888\n",
      "social+personal - Training Model with 4 neurons - 0 init\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Acc (4 hidden neurons, 0 init):  0.8888888888888888\n",
      "social+personal - Training Model with 5 neurons - 0 init\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Acc (5 hidden neurons, 0 init):  0.8888888888888888\n",
      "social+personal - Training Model with 6 neurons - 0 init\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Acc (6 hidden neurons, 0 init):  0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('kFold Training Process')\n",
    "\n",
    "if True: # remove when your are shure to spent some time!!!\n",
    "    basic_datasets = [dev_social, dev_personal]\n",
    "    basic_datasets_names = ['social', 'personal']\n",
    "    \n",
    "    for ifold in range(n_folds):\n",
    "        cv_name = 'kFoldsCV_fold_%i_indexes.pkl'%(ifold)\n",
    "        fusioned_data = []\n",
    "        with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "            [trn_idx,val_idx] = pickle.load(file_handler) \n",
    "        print('Fold %i - Processing basic datasets'%(ifold))\n",
    "        for idataset, dataset_name in enumerate(basic_datasets_names): \n",
    "            data = basic_datasets[idataset]\n",
    "            df_acc = pd.read_csv(os.path.join(model_path, '%s_kfold_model_acc.csv'%(dataset_name)),index_col=0)\n",
    "            df_status = pd.read_csv(os.path.join(model_path, '%s_kfold_model_status.csv'%(dataset_name)),index_col=0)\n",
    "            df_status = df_status.astype(bool)\n",
    "            df_names = pd.read_csv(os.path.join(model_path, '%s_kfold_model_names.csv'%(dataset_name)),index_col=0)\n",
    "            df_names = df_names.astype(str)\n",
    "\n",
    "            print('Fold %i - Processing %s'%(ifold, dataset_name))\n",
    "            pipe_name = 'kFoldsCV_fold_%i_pipe_%s.pkl'%(ifold, dataset_name)\n",
    "            with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "                preproc_pipe = joblib.load(file_handler)\n",
    "           \n",
    "            # get the number of neurons choose in analysis notebook and process data\n",
    "            if df_status.loc[ifold,str(datasets_n_neurons[idataset])] != True:\n",
    "                print('[ERROR] Train the model')\n",
    "            else:\n",
    "                # load model\n",
    "                print('%s - Loading Model with %i neuron(s): %s'%(dataset_name, datasets_n_neurons[idataset], \n",
    "                                                                df_names.loc[ifold, str(datasets_n_neurons[idataset])]))\n",
    "                model_name = df_names.loc[ifold, str(datasets_n_neurons[idataset])]\n",
    "                if os.path.exists(os.path.join(model_path, model_name)) == False:\n",
    "                    print('Please do the training process!!!')\n",
    "                    continue\n",
    "                else:\n",
    "                    model = MLPModel(n_hidden_neurons=datasets_n_neurons[idataset],verbose=0)\n",
    "                    model.load(os.path.join(model_path, model_name))\n",
    "            \n",
    "            nn_pipe = deepcopy(preproc_pipe)\n",
    "            nn_pipe.steps.append(('nn_%s_no_output'%(dataset_name), model.model_with_no_output_layer()))\n",
    "            \n",
    "            if dataset_name == 'social':\n",
    "                new_data = nn_pipe.predict(data)\n",
    "            else:\n",
    "                new_data = nn_pipe[:-1].transform(data)\n",
    "                new_data = nn_pipe[-1].predict(new_data)\n",
    "                \n",
    "            if fusioned_data == []:\n",
    "                fusioned_data = new_data\n",
    "            else:\n",
    "                fusioned_data = np.append(fusioned_data, new_data, axis=1)\n",
    "                \n",
    "    # training process using data processed with previous models\n",
    "    dataset_name = 'social+personal'\n",
    "    df_acc = pd.read_csv(os.path.join(model_path, '%s_kfold_model_acc.csv'%(dataset_name)),index_col=0)\n",
    "    df_status = pd.read_csv(os.path.join(model_path, '%s_kfold_model_status.csv'%(dataset_name)),index_col=0)\n",
    "    df_status = df_status.astype(bool)\n",
    "    df_names = pd.read_csv(os.path.join(model_path, '%s_kfold_model_names.csv'%(dataset_name)),index_col=0)\n",
    "    df_names = df_names.astype(str)\n",
    "\n",
    "    for ineuron in hidden_neurons:\n",
    "        best_acc = -1\n",
    "        best_init = -1\n",
    "        if df_status.loc[ifold,str(ineuron)] == False:\n",
    "            for iinit in range(1):\n",
    "                print('%s - Training Model with %i neurons - %i init'%(dataset_name, ineuron, iinit))\n",
    "                model_name = '%s_kFoldsCV_fold_%i_MLPModel_%i_hidden_neurons_%i_init.pkl'%(dataset_name, \n",
    "                                                                                                    ifold, ineuron,\n",
    "                                                                                                    iinit)\n",
    "                if os.path.exists(os.path.join(model_path, model_name)) == False:\n",
    "                    model = MLPModel(n_hidden_neurons=ineuron,verbose=0)\n",
    "                    model.fit(fusioned_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, epochs=1000, random_state=iinit)\n",
    "                    model.save(os.path.join(model_path, model_name))\n",
    "                else:\n",
    "                    model = MLPModel(n_hidden_neurons=ineuron,verbose=0)\n",
    "                    model.load(os.path.join(model_path, model_name))\n",
    "                model_acc = accuracy_score(trn_trgt,np.sign(model.predict(fusioned_data)))   \n",
    "                print('Acc (%i hidden neurons, %i init): '%(ineuron,iinit), model_acc)\n",
    "                if model_acc > best_acc:\n",
    "                    best_acc = model_acc\n",
    "                    best_init = iinit\n",
    "                    df_status.loc[ifold,str(ineuron)] = True\n",
    "                    df_acc.loc[ifold,str(ineuron)] = model_acc\n",
    "                    df_names.loc[ifold,str(ineuron)] = model_name\n",
    "                    df_acc.to_csv(os.path.join(model_path, '%s_kfold_model_acc.csv'%(dataset_name)))\n",
    "                    df_status.to_csv(os.path.join(model_path, '%s_kfold_model_status.csv'%(dataset_name)))\n",
    "                    df_names.to_csv(os.path.join(model_path, '%s_kfold_model_names.csv'%(dataset_name)))\n",
    "                del model\n",
    "        else:\n",
    "            print('Model training: done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c73ce",
   "metadata": {},
   "source": [
    "### (Social+Personal) + Clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a34df75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kFold Config Files Generation\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = [1, 2, 3, 4, 5, 6] # first hint!\n",
    "model_path = '../data/models'\n",
    "n_folds = 5\n",
    "datasets_n_neurons = [1, 1, 1, 1] # check the chose in analysis notebook\n",
    "\n",
    "print('kFold Config Files Generation')\n",
    "if True: # remove when your are shure to spent time!!!    \n",
    "    datasets_name = ['social+personal+clinical']\n",
    "    metrics = ['acc', 'sens', 'spec', 'sp']\n",
    "\n",
    "    buffer = 0*np.ones([n_folds,len(hidden_neurons)])\n",
    "    df_buffer = pd.DataFrame(data=buffer, columns=hidden_neurons,index=range(n_folds))\n",
    "\n",
    "    for idataset, dataset_name in enumerate(datasets_name):\n",
    "        for imetric in metrics:\n",
    "            df_buffer.to_csv(os.path.join(model_path, '%s_kfold_model_%s.csv'%(dataset_name,imetric)))\n",
    "    buffer = False*np.ones([n_folds,len(hidden_neurons)])\n",
    "    df_buffer = pd.DataFrame(data=buffer, columns=hidden_neurons,index=range(n_folds))\n",
    "    df_buffer.to_csv(os.path.join(model_path, '%s_kfold_model_status.csv'%(dataset_name)))\n",
    "    buffer = False*np.ones([n_folds,len(hidden_neurons)])\n",
    "    df_buffer = pd.DataFrame(data=buffer, columns=hidden_neurons,index=range(n_folds))\n",
    "    df_buffer.to_csv(os.path.join(model_path, '%s_kfold_model_names.csv'%(dataset_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a967609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8687642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8caf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18bf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f0ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c1fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

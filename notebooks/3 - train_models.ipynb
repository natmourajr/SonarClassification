{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db1e8ed",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa72844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /tf\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: src\n",
      "  Building wheel for src (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for src: filename=src-0.0.1-py3-none-any.whl size=14284 sha256=d5a69f9e209623303aa0343d864b64af098f242015d6e7d0046e9690970c80c7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-akoouvby/wheels/b8/4c/5d/041c4fc7e6c2e6d5dbdf9e9296283834aead4dadd6ecfdb44e\n",
      "Successfully built src\n",
      "Installing collected packages: src\n",
      "  Attempting uninstall: src\n",
      "    Found existing installation: src 0.0.1\n",
      "    Uninstalling src-0.0.1:\n",
      "      Successfully uninstalled src-0.0.1\n",
      "Successfully installed src-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61f352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "from src.functions.AuxiliarFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673cc4f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63c88a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_id</th>\n",
       "      <th>label</th>\n",
       "      <th>wav_files_path</th>\n",
       "      <th>processed_file_folder</th>\n",
       "      <th>processed_file_path</th>\n",
       "      <th>wav_files_info</th>\n",
       "      <th>cv_alg</th>\n",
       "      <th>cv_folds</th>\n",
       "      <th>cv_path</th>\n",
       "      <th>preproc_alg</th>\n",
       "      <th>...</th>\n",
       "      <th>pipeline_path</th>\n",
       "      <th>scaler_alg</th>\n",
       "      <th>train_data_path</th>\n",
       "      <th>train_trgt_path</th>\n",
       "      <th>target_label_file</th>\n",
       "      <th>target_id_file</th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_inits</th>\n",
       "      <th>model_neurons</th>\n",
       "      <th>model_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7182709064797668123</td>\n",
       "      <td>ShipsEar NN Classification</td>\n",
       "      <td>../data/shipsEar_AUDIOS</td>\n",
       "      <td>../data</td>\n",
       "      <td>../data/-7182709064797668123_processed_data.csv</td>\n",
       "      <td>../data/wav_file_informations.csv</td>\n",
       "      <td>StratifiedKFolds</td>\n",
       "      <td>5</td>\n",
       "      <td>../data/indexes</td>\n",
       "      <td>Lofar</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/pipelines</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>../data/-7182709064797668123_train_data.csv</td>\n",
       "      <td>../data/-7182709064797668123_trgt_data.csv</td>\n",
       "      <td>../data/models/train_label_file.csv</td>\n",
       "      <td>../data/models/train_id_file.csv</td>\n",
       "      <td>../data/models</td>\n",
       "      <td>1</td>\n",
       "      <td>../data/models/-7182709064797668123_hidden_neu...</td>\n",
       "      <td>../data/models/-7182709064797668123_model_stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1518950927628997345</td>\n",
       "      <td>Toy Data Classification</td>\n",
       "      <td>../data/shipsEar_AUDIOS</td>\n",
       "      <td>../data</td>\n",
       "      <td>../data/1518950927628997345_processed_data.csv</td>\n",
       "      <td>../data/wav_file_informations.csv</td>\n",
       "      <td>StratifiedKFolds</td>\n",
       "      <td>5</td>\n",
       "      <td>../data/indexes</td>\n",
       "      <td>MFCC</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/pipelines</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>../data/1518950927628997345_train_data.csv</td>\n",
       "      <td>../data/1518950927628997345_trgt_data.csv</td>\n",
       "      <td>../data/models/train_label_file.csv</td>\n",
       "      <td>../data/models/train_id_file.csv</td>\n",
       "      <td>../data/models</td>\n",
       "      <td>1</td>\n",
       "      <td>../data/models/1518950927628997345_hidden_neur...</td>\n",
       "      <td>../data/models/1518950927628997345_model_statu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               hash_id                       label           wav_files_path  \\\n",
       "0 -7182709064797668123  ShipsEar NN Classification  ../data/shipsEar_AUDIOS   \n",
       "1  1518950927628997345     Toy Data Classification  ../data/shipsEar_AUDIOS   \n",
       "\n",
       "  processed_file_folder                              processed_file_path  \\\n",
       "0               ../data  ../data/-7182709064797668123_processed_data.csv   \n",
       "1               ../data   ../data/1518950927628997345_processed_data.csv   \n",
       "\n",
       "                      wav_files_info            cv_alg  cv_folds  \\\n",
       "0  ../data/wav_file_informations.csv  StratifiedKFolds         5   \n",
       "1  ../data/wav_file_informations.csv  StratifiedKFolds         5   \n",
       "\n",
       "           cv_path preproc_alg  ...      pipeline_path      scaler_alg  \\\n",
       "0  ../data/indexes       Lofar  ...  ../data/pipelines  StandardScaler   \n",
       "1  ../data/indexes        MFCC  ...  ../data/pipelines  StandardScaler   \n",
       "\n",
       "                               train_data_path  \\\n",
       "0  ../data/-7182709064797668123_train_data.csv   \n",
       "1   ../data/1518950927628997345_train_data.csv   \n",
       "\n",
       "                              train_trgt_path  \\\n",
       "0  ../data/-7182709064797668123_trgt_data.csv   \n",
       "1   ../data/1518950927628997345_trgt_data.csv   \n",
       "\n",
       "                     target_label_file                    target_id_file  \\\n",
       "0  ../data/models/train_label_file.csv  ../data/models/train_id_file.csv   \n",
       "1  ../data/models/train_label_file.csv  ../data/models/train_id_file.csv   \n",
       "\n",
       "       model_path model_inits  \\\n",
       "0  ../data/models           1   \n",
       "1  ../data/models           1   \n",
       "\n",
       "                                       model_neurons  \\\n",
       "0  ../data/models/-7182709064797668123_hidden_neu...   \n",
       "1  ../data/models/1518950927628997345_hidden_neur...   \n",
       "\n",
       "                                        model_status  \n",
       "0  ../data/models/-7182709064797668123_model_stat...  \n",
       "1  ../data/models/1518950927628997345_model_statu...  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_config = pd.read_csv('../data/config.csv')\n",
    "train_id = 1\n",
    "df_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f58e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(df_config['train_data_path'][train_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24cd7f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.809787</td>\n",
       "      <td>1.140782</td>\n",
       "      <td>0.552740</td>\n",
       "      <td>1.942452</td>\n",
       "      <td>-0.099175</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.275852</td>\n",
       "      <td>1.482409</td>\n",
       "      <td>0.423330</td>\n",
       "      <td>1.247347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699771</td>\n",
       "      <td>0.220017</td>\n",
       "      <td>-0.916476</td>\n",
       "      <td>0.515953</td>\n",
       "      <td>0.282666</td>\n",
       "      <td>0.945631</td>\n",
       "      <td>0.277907</td>\n",
       "      <td>1.574058</td>\n",
       "      <td>0.042095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.582713</td>\n",
       "      <td>3.161590</td>\n",
       "      <td>0.083602</td>\n",
       "      <td>-2.986270</td>\n",
       "      <td>1.151863</td>\n",
       "      <td>2.075501</td>\n",
       "      <td>-0.596150</td>\n",
       "      <td>-0.524811</td>\n",
       "      <td>1.258575</td>\n",
       "      <td>-1.426467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218064</td>\n",
       "      <td>0.154101</td>\n",
       "      <td>0.137039</td>\n",
       "      <td>-1.850546</td>\n",
       "      <td>2.839435</td>\n",
       "      <td>0.895683</td>\n",
       "      <td>0.104921</td>\n",
       "      <td>2.725624</td>\n",
       "      <td>-0.170036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.388922</td>\n",
       "      <td>-0.312243</td>\n",
       "      <td>0.332491</td>\n",
       "      <td>-1.109370</td>\n",
       "      <td>0.389026</td>\n",
       "      <td>-0.028511</td>\n",
       "      <td>0.292879</td>\n",
       "      <td>-3.008659</td>\n",
       "      <td>-0.940198</td>\n",
       "      <td>-2.708882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232607</td>\n",
       "      <td>-1.320325</td>\n",
       "      <td>-0.563335</td>\n",
       "      <td>0.552087</td>\n",
       "      <td>1.852921</td>\n",
       "      <td>-0.539202</td>\n",
       "      <td>0.366420</td>\n",
       "      <td>0.859907</td>\n",
       "      <td>-0.567231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.299062</td>\n",
       "      <td>1.340423</td>\n",
       "      <td>-0.705663</td>\n",
       "      <td>0.243180</td>\n",
       "      <td>-0.178422</td>\n",
       "      <td>-2.069673</td>\n",
       "      <td>-0.533815</td>\n",
       "      <td>-2.986325</td>\n",
       "      <td>-0.977363</td>\n",
       "      <td>-1.303734</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.848854</td>\n",
       "      <td>-1.474998</td>\n",
       "      <td>-1.686704</td>\n",
       "      <td>2.852161</td>\n",
       "      <td>3.344909</td>\n",
       "      <td>-0.228653</td>\n",
       "      <td>0.191883</td>\n",
       "      <td>0.918660</td>\n",
       "      <td>-0.124210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.559000</td>\n",
       "      <td>-0.551521</td>\n",
       "      <td>2.361235</td>\n",
       "      <td>-2.182949</td>\n",
       "      <td>-0.161701</td>\n",
       "      <td>-0.329970</td>\n",
       "      <td>0.588126</td>\n",
       "      <td>-2.942822</td>\n",
       "      <td>0.375951</td>\n",
       "      <td>-1.165616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.030261</td>\n",
       "      <td>-2.005482</td>\n",
       "      <td>0.872376</td>\n",
       "      <td>-1.896221</td>\n",
       "      <td>2.920265</td>\n",
       "      <td>1.582279</td>\n",
       "      <td>1.642354</td>\n",
       "      <td>0.126912</td>\n",
       "      <td>0.468424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0  -0.809787   1.140782   0.552740   1.942452  -0.099175   0.122905   \n",
       "1   2.582713   3.161590   0.083602  -2.986270   1.151863   2.075501   \n",
       "2   3.388922  -0.312243   0.332491  -1.109370   0.389026  -0.028511   \n",
       "3   2.299062   1.340423  -0.705663   0.243180  -0.178422  -2.069673   \n",
       "4  -1.559000  -0.551521   2.361235  -2.182949  -0.161701  -0.329970   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  ...  feature_11  feature_12  \\\n",
       "0   0.275852   1.482409   0.423330   1.247347  ...   -0.699771    0.220017   \n",
       "1  -0.596150  -0.524811   1.258575  -1.426467  ...   -0.218064    0.154101   \n",
       "2   0.292879  -3.008659  -0.940198  -2.708882  ...   -0.232607   -1.320325   \n",
       "3  -0.533815  -2.986325  -0.977363  -1.303734  ...   -1.848854   -1.474998   \n",
       "4   0.588126  -2.942822   0.375951  -1.165616  ...    1.030261   -2.005482   \n",
       "\n",
       "   feature_13  feature_14  feature_15  feature_16  feature_17  feature_18  \\\n",
       "0   -0.916476    0.515953    0.282666    0.945631    0.277907    1.574058   \n",
       "1    0.137039   -1.850546    2.839435    0.895683    0.104921    2.725624   \n",
       "2   -0.563335    0.552087    1.852921   -0.539202    0.366420    0.859907   \n",
       "3   -1.686704    2.852161    3.344909   -0.228653    0.191883    0.918660   \n",
       "4    0.872376   -1.896221    2.920265    1.582279    1.642354    0.126912   \n",
       "\n",
       "   feature_19  target  \n",
       "0    0.042095       1  \n",
       "1   -0.170036       1  \n",
       "2   -0.567231       0  \n",
       "3   -0.124210       0  \n",
       "4    0.468424       0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a57ede",
   "metadata": {},
   "source": [
    "## Fit a preprocessing pipeline for each kFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d514943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "index_path = df_config['cv_path'][0]\n",
    "pipe_path = df_config['pipeline_path'][0]\n",
    "n_folds =  df_config['cv_folds'][0]\n",
    "\n",
    "data = df_train.drop(columns=['target'])\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    index_file = '%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                       ifold, n_folds)\n",
    "    with open(os.path.join(index_path,index_file),'rb') as file_handler:\n",
    "            [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "    \n",
    "    # criando o pipeline\n",
    "    scaler = None\n",
    "    if df_config['scaler_alg'][train_id] == \"StandardScaler\":\n",
    "        scaler = StandardScaler()\n",
    "    elif df_config['scaler_alg'][train_id] == \"MinMaxScaler\":\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    pipe = Pipeline(steps=[(\"scaler\", scaler)])\n",
    "    pipe.fit(data.loc[trn_idx,:])\n",
    "    \n",
    "    pipe_name = '%s_CV_fold_%i_of_%i_cv_pipe.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                   ifold, n_folds)\n",
    "    with open(os.path.join(pipe_path,pipe_name),'wb') as file_handler:\n",
    "        joblib.dump(pipe, file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f34dd3",
   "metadata": {},
   "source": [
    "# Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8394424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel:\n",
    "    def __init__(self, n_hidden_neurons=2, verbose=2):\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.model = None\n",
    "        self.trn_history = None\n",
    "        self.trained = False\n",
    "        self.verbose = verbose\n",
    "    def __str__(self):\n",
    "        m_str = 'Class MLPModel\\n'\n",
    "        if self.trained:\n",
    "            m_str += 'Model is fitted, '\n",
    "        else:\n",
    "            m_str += 'Model is not fitted, '\n",
    "        m_str += 'instance created with %i hidden neurons'%(self.n_hidden_neurons) \n",
    "        return m_str\n",
    "    def create_model(self, data, target, random_state=0, learning_rate=0.01):\n",
    "        #tf.random.set_seed(random_state)\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # add a input to isolate the input of NN model\n",
    "        model.add(tf.keras.Input(shape=(data.shape[1],)))\n",
    "        # add a non-linear single neuron layer\n",
    "        hidden_layer = layers.Dense(units=self.n_hidden_neurons,\n",
    "                                    activation='tanh',\n",
    "                                    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                                    kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                                    bias_regularizer=regularizers.L2(1e-4),\n",
    "                                    bias_initializer=initializers.Zeros()\n",
    "                                   )\n",
    "        model.add(hidden_layer)\n",
    "        # add a non-linear output layer with max sparse target shape\n",
    "        output_layer = layers.Dense(units=target.shape[1],\n",
    "                                    activation='tanh',\n",
    "                                    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                                    bias_initializer=initializers.Zeros()\n",
    "                                   )\n",
    "        model.add(output_layer)\n",
    "        # creating a optimization function using steepest gradient\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                                  decay_steps=100,\n",
    "                                                                  decay_rate=0.9)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "        # compile the model\n",
    "        loss = keras.losses.CategoricalCrossentropy(from_logits=False,\n",
    "                                                    label_smoothing=0.0,\n",
    "                                                    axis=-1,\n",
    "                                                    reduction=\"auto\",\n",
    "                                                    name=\"cat_crossent\",)\n",
    "\n",
    "\n",
    "        cat_cross = keras.losses.BinaryCrossentropy(from_logits=False,\n",
    "                                                         label_smoothing=0.0,\n",
    "                                                         axis=-1,\n",
    "                                                         reduction=\"auto\",\n",
    "                                                         name=\"cat_crossent_met\",)\n",
    "        cat_acc_metric = keras.metrics.BinaryAccuracy(name=\"cat_acc\", dtype=None)\n",
    "        acc_metric = keras.metrics.Accuracy(name=\"accuracy\",dtype=None)\n",
    "        mse_metric = keras.metrics.MeanSquaredError(name=\"mse\", dtype=None)\n",
    "        rmse_metric = keras.metrics.RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
    "\n",
    "        model.compile(loss=\"mean_squared_error\", \n",
    "                      optimizer=optimizer,\n",
    "                      metrics=[acc_metric,mse_metric,rmse_metric])\n",
    "        return model\n",
    "    def fit(self, X, Y,\n",
    "            trn_id=None, \n",
    "            val_id=None, \n",
    "            epochs=50,\n",
    "            batch_size=4,\n",
    "            patience = 100,\n",
    "            learning_rate=0.01, random_state=0):\n",
    "        \n",
    "        X_copy = X.copy()\n",
    "        Y_copy = Y.copy()\n",
    "        \n",
    "        model = self.create_model(X_copy,Y_copy, random_state=random_state, learning_rate=learning_rate)\n",
    "        \n",
    "        # early stopping to avoid overtraining\n",
    "        earlyStopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                patience=patience,verbose=self.verbose, \n",
    "                                                mode='auto')\n",
    "    \n",
    "        trn_desc = model.fit(X_copy[trn_id,:], Y_copy[trn_id],\n",
    "                             epochs=epochs,\n",
    "                             batch_size=batch_size,\n",
    "                             callbacks=[earlyStopping], \n",
    "                             verbose=self.verbose,\n",
    "                             validation_data=(X_copy[val_id,:],\n",
    "                                              Y_copy[val_id]),\n",
    "                            )\n",
    "        self.model = model\n",
    "        self.trn_history = trn_desc\n",
    "        self.trained = True\n",
    "    def predict(self, data):\n",
    "        return self.model.predict(data)\n",
    "    def save(self, file_path):\n",
    "        with open(file_path,'wb') as file_handler:\n",
    "            joblib.dump([self.n_hidden_neurons, self.model,\n",
    "                        self.trn_history, self.trained], file_handler)\n",
    "    def load(self, file_path):\n",
    "        with open(file_path,'rb') as file_handler:\n",
    "            [self.n_hidden_neurons, self.model, self.trn_history, self.trained]= joblib.load(file_handler)\n",
    "    def model_with_no_output_layer(self):\n",
    "        buffer_model = tf.keras.Sequential()    \n",
    "        # add a input to isolate the input of NN model\n",
    "        buffer_model.add(tf.keras.Input(shape=(model.model.layers[0].get_weights()[0].shape[0],)))\n",
    "        # add a non-linear single neuron layer\n",
    "        hidden_layer = layers.Dense(units=model.model.layers[0].get_weights()[1].shape[0],\n",
    "                                    activation='tanh')\n",
    "        buffer_model.add(hidden_layer)    \n",
    "        output_layer = layers.Dense(units=1,activation='tanh')\n",
    "    \n",
    "        for idx, layer in enumerate(buffer_model.layers):\n",
    "            layer.set_weights(model.model.layers[idx].get_weights())\n",
    "        return buffer_model\n",
    "    def predict_one_layer_before_output(self, data):\n",
    "        buffer_model = self.model_with_no_output_layer()\n",
    "        return buffer_model.predict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029814be",
   "metadata": {},
   "source": [
    "# kFold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2edf93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_description(df_config, train_id):\n",
    "    str_out = '\\n\\n\\n'\n",
    "    str_out +=  '=======================================\\n'\n",
    "    str_out +=  '%s Training Process'%(df_config['label'][train_id])\n",
    "    str_out += '\\n'\n",
    "    str_out +=  '=======================================\\n'\n",
    "    \n",
    "    str_out += 'Processing %s'%(df_config['train_data_path'][train_id])+'\\n'\n",
    "    str_out += 'Hidden Neurons:'\n",
    "    hidden_neurons = ' '\n",
    "    for ihidden_neuron in list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id])):\n",
    "        hidden_neurons += str(ihidden_neuron)+', '\n",
    "    str_out += hidden_neurons[:-2]\n",
    "    str_out += '\\n'\n",
    "    str_out += 'CV Folds: %s\\n'%(df_config['cv_folds'][train_id])\n",
    "    str_out += 'Inits: %s\\n'%(df_config['model_inits'][train_id])\n",
    "    return str_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8d7091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel:\n",
    "    def __init__(self, n_hidden_neurons=2, verbose=2):\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.model = None\n",
    "        self.trn_history = None\n",
    "        self.trained = False\n",
    "        self.verbose = verbose\n",
    "    def __str__(self):\n",
    "        m_str = 'Class MLPModel\\n'\n",
    "        if self.trained:\n",
    "            m_str += 'Model is fitted, '\n",
    "        else:\n",
    "            m_str += 'Model is not fitted, '\n",
    "        m_str += 'instance created with %i hidden neurons'%(self.n_hidden_neurons) \n",
    "        return m_str\n",
    "    def loss_function(self):\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=False,\n",
    "            label_smoothing=0.0,\n",
    "            axis=-1,\n",
    "            reduction=\"auto\",\n",
    "            name=\"binary_crossentropy\",\n",
    "        )\n",
    "        return loss\n",
    "        \n",
    "    def create_model(self, data, target, random_state=0, learning_rate=0.01):\n",
    "        #tf.random.set_seed(random_state)\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # add a input to isolate the input of NN model\n",
    "        model.add(tf.keras.Input(shape=(data.shape[1],)))\n",
    "        # add a non-linear single neuron layer\n",
    "        hidden_layer = layers.Dense(units=self.n_hidden_neurons,\n",
    "                                    activation='tanh',\n",
    "                                    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                                    kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                                    bias_regularizer=regularizers.L2(1e-4),\n",
    "                                    bias_initializer=initializers.Zeros()\n",
    "                                   )\n",
    "        model.add(hidden_layer)\n",
    "        # add a non-linear output layer with max sparse target shape\n",
    "        output_layer = layers.Dense(units=target.shape[1],\n",
    "                                    activation='tanh',\n",
    "                                    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                                    bias_initializer=initializers.Zeros()\n",
    "                                   )\n",
    "        model.add(output_layer)\n",
    "        # creating a optimization function using steepest gradient\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                                  decay_steps=100,\n",
    "                                                                  decay_rate=0.9)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False,\n",
    "                                amsgrad=False, weight_decay=None, clipnorm=None,\n",
    "                                clipvalue=None,global_clipnorm=None,use_ema=False,\n",
    "                                ema_momentum=0.99,ema_overwrite_frequency=None,\n",
    "                                jit_compile=True,name=\"SGD\",)\n",
    "        \n",
    "        \n",
    "        # compile the model\n",
    "        loss = keras.losses.CategoricalCrossentropy(from_logits=True,\n",
    "                                                    label_smoothing=0.0,\n",
    "                                                    axis=-1,\n",
    "                                                    reduction=\"auto\",\n",
    "                                                    name=\"cat_crossent\",)\n",
    "\n",
    "        loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "       \n",
    "        cat_acc_metric = keras.metrics.CategoricalAccuracy(name=\"cat_acc\", dtype=None)\n",
    "        acc_metric = keras.metrics.Accuracy(name=\"accuracy\",dtype=None)\n",
    "        mse_metric = keras.metrics.MeanSquaredError(name=\"mse\", dtype=None)\n",
    "        rmse_metric = keras.metrics.RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
    "\n",
    "        model.compile(loss=loss, \n",
    "                      optimizer=optimizer,\n",
    "                      metrics=[cat_acc_metric,acc_metric,mse_metric,rmse_metric])\n",
    "        return model\n",
    "    def fit(self, X, Y,\n",
    "            trn_id=None, \n",
    "            val_id=None, \n",
    "            epochs=50,\n",
    "            batch_size=4,\n",
    "            patience = 100,\n",
    "            learning_rate=0.01, random_state=0):\n",
    "        \n",
    "        X_copy = X.copy()\n",
    "        Y_copy = Y.copy()\n",
    "        \n",
    "        model = self.create_model(X_copy,Y_copy, random_state=random_state, learning_rate=learning_rate)\n",
    "        \n",
    "        # early stopping to avoid overtraining\n",
    "        earlyStopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                patience=patience,verbose=self.verbose, \n",
    "                                                mode='auto')\n",
    "    \n",
    "        trn_desc = model.fit(X_copy[trn_id,:], Y_copy[trn_id],\n",
    "                             epochs=epochs,\n",
    "                             batch_size=batch_size,\n",
    "                             callbacks=[earlyStopping], \n",
    "                             verbose=self.verbose,\n",
    "                             validation_data=(X_copy[val_id,:],\n",
    "                                              Y_copy[val_id]),\n",
    "                            )\n",
    "        self.model = model\n",
    "        self.trn_history = trn_desc\n",
    "        self.trained = True\n",
    "    def predict(self, data):\n",
    "        return self.model.predict(data)\n",
    "    def save(self, file_path):\n",
    "        with open(file_path,'wb') as file_handler:\n",
    "            joblib.dump([self.n_hidden_neurons, self.model,\n",
    "                        self.trn_history, self.trained], file_handler)\n",
    "    def load(self, file_path):\n",
    "        with open(file_path,'rb') as file_handler:\n",
    "            [self.n_hidden_neurons, self.model, self.trn_history, self.trained]= joblib.load(file_handler)\n",
    "    def model_with_no_output_layer(self):\n",
    "        buffer_model = tf.keras.Sequential()    \n",
    "        # add a input to isolate the input of NN model\n",
    "        buffer_model.add(tf.keras.Input(shape=(model.model.layers[0].get_weights()[0].shape[0],)))\n",
    "        # add a non-linear single neuron layer\n",
    "        hidden_layer = layers.Dense(units=model.model.layers[0].get_weights()[1].shape[0],\n",
    "                                    activation='tanh')\n",
    "        buffer_model.add(hidden_layer)    \n",
    "        output_layer = layers.Dense(units=1,activation='tanh')\n",
    "    \n",
    "        for idx, layer in enumerate(buffer_model.layers):\n",
    "            layer.set_weights(model.model.layers[idx].get_weights())\n",
    "        return buffer_model\n",
    "    def predict_one_layer_before_output(self, data):\n",
    "        buffer_model = self.model_with_no_output_layer()\n",
    "        return buffer_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2c4e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "=======================================\n",
      "Toy Data Classification Training Process\n",
      "=======================================\n",
      "Processing ../data/1518950927628997345_train_data.csv\n",
      "Hidden Neurons: 1\n",
      "CV Folds: 5\n",
      "Inits: 1\n",
      "\n",
      "Training Starting\n",
      "Data shape: (10000, 20)\n",
      "Trgt shape: 10000\n",
      "Training 1 fold of 5 folds\n",
      "Training for 1 neuron in [1]\n",
      "Training for 1 init in 1 inits\n",
      "Modelo não existe\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected keyword argument passed to optimizer: amsgrad. Allowed kwargs are {'global_clipnorm', 'lr', 'clipnorm', 'decay', 'clipvalue'}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModelo não existe\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPModel(n_hidden_neurons\u001b[38;5;241m=\u001b[39mineuron,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_trgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrn_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miinit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#model.save(os.path.join(model_path, model_name))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [32], line 91\u001b[0m, in \u001b[0;36mMLPModel.fit\u001b[0;34m(self, X, Y, trn_id, val_id, epochs, batch_size, patience, learning_rate, random_state)\u001b[0m\n\u001b[1;32m     88\u001b[0m X_copy \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     89\u001b[0m Y_copy \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 91\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# early stopping to avoid overtraining\u001b[39;00m\n\u001b[1;32m     94\u001b[0m earlyStopping \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     95\u001b[0m                                         patience\u001b[38;5;241m=\u001b[39mpatience,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \n\u001b[1;32m     96\u001b[0m                                         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [32], line 55\u001b[0m, in \u001b[0;36mMLPModel.create_model\u001b[0;34m(self, data, target, random_state, learning_rate)\u001b[0m\n\u001b[1;32m     50\u001b[0m lr_schedule \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mschedules\u001b[38;5;241m.\u001b[39mExponentialDecay(initial_learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m     51\u001b[0m                                                           decay_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     52\u001b[0m                                                           decay_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m     53\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m---> 55\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mjit_compile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSGD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# compile the model\u001b[39;00m\n\u001b[1;32m     63\u001b[0m loss \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m                                             label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     65\u001b[0m                                             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     66\u001b[0m                                             reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m                                             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_crossent\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[0;34m(self, learning_rate, momentum, nesterov, name, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    103\u001b[0m              learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m    104\u001b[0m              momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    105\u001b[0m              nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m              name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    107\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 108\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_hyper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m, learning_rate))\n\u001b[1;32m    110\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_hyper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_decay)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:351\u001b[0m, in \u001b[0;36mOptimizerV2.__init__\u001b[0;34m(self, name, gradient_aggregator, gradient_transformers, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    350\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_kwargs:\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected keyword argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to optimizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(k)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Allowed kwargs are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    354\u001b[0m   \u001b[38;5;66;03m# checks that all keyword arguments are non-negative.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs[k] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m kwargs[k] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected keyword argument passed to optimizer: amsgrad. Allowed kwargs are {'global_clipnorm', 'lr', 'clipnorm', 'decay', 'clipvalue'}."
     ]
    }
   ],
   "source": [
    "# for kFolds CV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from src.functions.AuxiliarFunctions import *\n",
    "\n",
    "\n",
    "print(get_train_description(df_config, train_id))\n",
    "\n",
    "\n",
    "if True: # remova quando tiver segurança no treinamento\n",
    "    print('Training Starting')\n",
    "    # data\n",
    "    model_type = 'MLPNeuralNetwork'\n",
    "    data = df_train.drop(columns=['target']).copy(deep=True)\n",
    "    print('Data shape: (%i, %i)'%(data.shape[0], data.shape[1]))\n",
    "    \n",
    "    trgt = df_train['target'].copy(deep=True).values\n",
    "    print('Trgt shape: %i'%(trgt.shape[0]))\n",
    "    \n",
    "    hidden_neurons = list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id]))\n",
    "    \n",
    "    with open(df_config['model_status'][train_id],'rb') as file_handler:\n",
    "        [model_status] = pickle.load(file_handler)\n",
    "    \n",
    "    \n",
    "    n_folds = df_config['cv_folds'][train_id]\n",
    "    for ifold in range(1):\n",
    "        print('Training %i fold of %i folds'%(ifold+1, n_folds))\n",
    "        # pipeline\n",
    "        pipeline_name = '%s_CV_fold_%i_of_%i_cv_pipe.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                           ifold, n_folds)\n",
    "        \n",
    "        pipeline_path = df_config['pipeline_path'][train_id]\n",
    "        \n",
    "        with open(os.path.join(pipeline_path,pipeline_name),'rb') as file_handler:\n",
    "            pipe = joblib.load(file_handler)\n",
    "        trn_data = pipe.transform(data)\n",
    "        trn_trgt = tf.keras.utils.to_categorical(trgt, num_classes=len(np.unique(trgt)))\n",
    "        \n",
    "        for idx, ineuron in enumerate(hidden_neurons):\n",
    "            print('Training for %i neuron in'%(ineuron),hidden_neurons)\n",
    "            \n",
    "            for iinit in range(df_config['model_inits'][train_id]):\n",
    "                print('Training for %i init in %i inits'%(iinit+1, df_config['model_inits'][train_id]))\n",
    "                model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                          model_type,ifold, ineuron, iinit)\n",
    "                model_path = df_config['model_path'][train_id]\n",
    "                #print(os.path.join(model_path, model_name))\n",
    "                if os.path.exists(os.path.join(model_path, model_name)):\n",
    "                    print('Modelo existente')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('Modelo não existe\\n\\n')\n",
    "                    model = MLPModel(n_hidden_neurons=ineuron,verbose=2)\n",
    "                    model.fit(trn_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, \n",
    "                              epochs=10, random_state=iinit, learning_rate=0.2)\n",
    "                    #model.save(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7724084a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
